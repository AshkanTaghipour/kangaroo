<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Kangaroo project page.">
  <meta name="keywords" content="Kangaroo, Pose-to-Video, Video Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Kangaroo: Text-to-Skeleton Cascades for Controllable Complex Human Motion Video Generation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <!-- FontAwesome icons (as in Nerfies template) -->
  <script defer src="./static/js/fontawesome.all.min.js"></script>

  <style>
    .muted-note { color: rgba(0,0,0,0.65); font-size: 0.95rem; }
    .btn-placeholder { cursor: default; opacity: 0.92; }

    .diagram-card {
      border-radius: 14px;
      overflow: hidden;
      background: #fff;
      box-shadow: 0 6px 18px rgba(0,0,0,0.06);
      margin-bottom: 1.25rem;
    }
    .diagram-card img {
      width: 100%;
      height: auto;
      display: block;
      background: #fff;
    }
    .diagram-caption {
      padding: 0.75rem 0.9rem;
      border-top: 1px solid rgba(0,0,0,0.06);
      font-weight: 600;
    }

    /* --- Gallery layout tweaks to remove side black margins --- */

    /* Center items in the gallery row */
    #result-gallery {
      justify-content: center;
    }

    /* Each column centers its card */
    .gallery-col {
      display: flex;
      justify-content: center;
    }

    /* Shrink-wrap card to content (video width) so no extra side space */
    .gallery-card {
      border-radius: 14px;
      overflow: hidden;
      background: #fff;
      box-shadow: 0 6px 18px rgba(0,0,0,0.06);
      display: inline-block;
      width: fit-content;
      max-width: 100%;
    }

    /* Video wrapper (no forced black margins) */
    .video-wrap{
      background: transparent;
      display:flex;
      justify-content:center;
      padding: 0;
    }

    /* Fit full tall video within screen height */
    .gallery-video{
      width: auto;
      max-width: 100%;
      height: auto;
      max-height: 80vh;
      object-fit: contain;
      display:block;
      background: transparent;
    }
  </style>
</head>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-1 publication-title">
            Kangaroo: Text-to-Skeleton Cascades for Controllable Complex Human Motion Video Generation
          </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Ashkan Taghipour<sup>1</sup>,</span>
            <span class="author-block">Morteza Ghahremani<sup>2</sup>,</span>
            <span class="author-block">Zinuo Li<sup>1</sup>,</span>
            <span class="author-block">Hamid Laga<sup>3</sup>,</span>
            <span class="author-block">Farid Boussaid<sup>1</sup>,</span>
            <span class="author-block">Mohammed Bennamoun<sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The University of Western Australia</span>
            <span class="author-block"><sup>2</sup>Technical University of Munich</span>
            <span class="author-block"><sup>3</sup>Murdoch University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <span class="link-block">
                <span class="external-link button is-normal is-rounded is-dark btn-placeholder">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv (coming soon)</span>
                </span>
              </span>

              <span class="link-block">
                <span class="external-link button is-normal is-rounded is-dark btn-placeholder">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code (coming soon)</span>
                </span>
              </span>

              <span class="link-block">
                <span class="external-link button is-normal is-rounded is-dark btn-placeholder">
                  <span class="icon"><i class="far fa-images"></i></span>
                  <span>Data (coming soon)</span>
                </span>
              </span>

            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Modern video diffusion models produce impressive results for text-to-video and image-to-video generation,
            yet they still struggle with complex, highly dynamic human motions such as flips, cartwheels, acrobatics,
            and martial arts. These motions are non-repetitive, involve large pose changes and frequent self-occlusions,
            and often trigger failure modes including implausible limb trajectories, temporal inconsistency, and identity
            drift, particularly when the input is a reference image and appearance preservation is essential. To address
            these challenges, we propose a two-stage cascaded framework that treats language as a high-level motion prior
            rather than a weak global control signal. In the first stage, an autoregressive text-to-skeleton model
            translates a natural-language description into a temporally coherent sequence of 2D poses by predicting each
            frame conditioned on previous poses and the text, capturing long-range dependencies and physically plausible
            trajectories without manual pose authoring. In the second stage, a pose-conditioned video diffusion model
            synthesizes videos from a reference image and the generated skeleton sequence, using a DiNOv3-based multi-level
            reference encoder with learnable feature fusion to better preserve identity and clothing details under large
            pose variations. Finally, to enable rigorous training and evaluation in this regime, we introduce an ethically
            sourced Blender-based synthetic dataset of 2,000 videos spanning diverse acrobatic and stunt-like actions,
            characters, and environments. Extensive experiments demonstrate improved motion fidelity, temporal consistency,
            and identity preservation for complex human motion generation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>

        <div class="diagram-card">
          <img src="./static/diagrams/motion_diagram.png" alt="Motion diagram">
          <div class="diagram-caption">Motion diagram</div>
        </div>

        <div class="diagram-card">
          <img src="./static/diagrams/video_diagram.png" alt="Video diagram">
          <div class="diagram-caption">Video diagram</div>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- Result Gallery -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Result Gallery</h2>

        <div id="gallery-status" class="content has-text-centered"></div>
        <div class="columns is-multiline" id="result-gallery"></div>
      </div>
    </div>
  </div>
</section>

<script>
  const VIDEO_FILES = [
    "Armada_unified_video_elon.mp4",
    "Armada_unified_video_messi.mp4",
    "Armada_unified_video_trump.mp4",
    "breakdance_footwork_unified_video_elon.mp4",
    "breakdance_footwork_unified_video_obama.mp4",
    "breakdance_freeze_unified_video_elon.mp4",
    "breakdance_freeze_unified_video_messi.mp4",
    "capoeira_unified_video_elon.mp4",
    "capoeira_unified_video_messi.mp4",
    "capoeira_unified_video_trump.mp4",
    "macaco_flip_unified_video_messi.mp4",
    "macaco_flip_unified_video_obama.mp4",
    "queshada_unified_video_taylor.mp4",
    "queshada_unified_video_trump.mp4",
    "spine_flip_unified_video_taylor.mp4",
    "spine_flip_unified_video_trump.mp4"
  ];

  function renderGallery() {
    const status = document.getElementById("gallery-status");
    const gallery = document.getElementById("result-gallery");

    if (!VIDEO_FILES.length) {
      status.textContent = "No videos configured.";
      return;
    }

    status.textContent = "";
    gallery.innerHTML = "";

    for (const fn of VIDEO_FILES) {
      const src = `./static/pose_to_video/${fn}`;

      const col = document.createElement("div");
      col.className = "column is-half gallery-col";

      col.innerHTML = `
        <div class="gallery-card">
          <div class="video-wrap">
            <video class="gallery-video" controls playsinline preload="metadata">
              <source src="${src}" type="video/mp4">
            </video>
          </div>
          <div class="content" style="padding: 0.6rem 0.9rem;">
            <a href="${src}" target="_blank" rel="noopener">Open video</a>
          </div>
        </div>
      `;

      gallery.appendChild(col);
    }
  }

  renderGallery();
</script>

</body>
</html>
